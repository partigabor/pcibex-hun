// Notes on Hungarian Experiment

# Experiment

## Design

- Show the 2 images (a and b) at the same time
- Show the sentence word by word (SPR)
- After the sentence, ask the participant to choose between the 2 images

## Record data

- RT of each word/region in the sentence
- RT of decision (from fixation time end to choice)
- Choice of image (a or b)
- Eye tracking data (gaze on regions, durations, etc.)

## Results and their interpretation

- What does the RT of each word/region tell us?
- What does the RT of decision tell us?
- What does the choice of image tell us?
- How do the RTs and choices relate to each other?
- What does the eye tracking data tell us...

# Literature

## Visual World Paradigm
- Almost exclusively used in speech processing studies with audio stimuli

## Self-paced reading

## Sentence comprehension / verification

## Sentence--picture verification

# TODO

## Counterbalancing

- shuffle vs rshuffle vs randomize
- hardcode shuffling of images

## Fillers

- Gender? Hungarian Å‘ (he/she)
- Noun phrases?
- Every one

Every kid climbs a tree
Minden gyerek egy fara maszott.
Minden gyerek fara maszott.
A kid climbed every tree

# Q&A

mouse?

# Notes

adjunct
modification structure
super big model

2 sets.

1st is the generic masculine, no gender bias sentences, only checking verbal reference.

2nd one has gender, with male and femalse subjects, and we switch up.

verbs, self or other directed verbs

2 variable: 
1. verbal preference, 
2. gender preference,

if we change the gender or status of subjects, would the interpretation of the sentence change?
examining verbal ambiguitz


###########################################

I have an eye tracking experiment built in PCIbex (https://doc.pcibex.net/), which is an experiment controller based on JavasSript. The experiment is about reading Hungarian sentences in a self-paced reading, then making a binary choice by selecting the appropriate image from two choices on the left and right sides of the screen. During the selection phase, the eye-tracker is activated. The behavioral data and the eyetracking data are in different files, let's handle the behavioral data first. My experiment code is in #file:main.js, the data is in #file:experiment_data.csv. After participants finish my experiment, I have the results in a terrible, hard-to-read format, in #file:results_dev.csv. This is not a true csv, so it needs some refactoring. I want python code that can parse the results in the #file:results.ipynb that turns this csv into a pandas dataframe, and transform it into a meaningful format, so that I can study every participants' events so it's easier to read. For example, before every submission, I have some metadata as comment like this:

# Results on Thu, 18 Sep 2025 11:52:32 GMT
# USER AGENT: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36
# Design number was non-random = 15

Then, before rows for specific "trial" events, there are the column names in comments before the data, such as:
# 1. Results reception time.
# 2. MD5 hash of participant's IP address.
# 3. Controller name.
# 4. Order number of item.
# 5. Inner element number.
# 6. Label.
# 7. Latin Square Group.
# 8. PennElementType.
# 9. PennElementName.
# 10. Parameter.
# 11. Value.
# 12. EventTime.
# 13. prolific_id.
# 14. Comments.

From these, the important ones are: The results reception time, save it as a timestamp value. The MD5 has of participant's IP address, save it also as 'MD5'. The Label, save it as label, this helps to know where we are in the experiment. Save also the rows where the PennElement is EyeTracker, and return the calibration or file value, and when did this happen (EventTime). (The file value will help us later to automatically download the eyetracking data from the server, for now it is provided here in root.)

After these initial data rows, I have the trial pages, such as where the participant enters the participant_id known, in this case "roro101". 

When the experiments start, I have additional logged info for every event, such as:
# 14. participant_id.
# 15. group.
# 16. no.
# 17. item.
# 18. condition.
# 19. counterbalancing.
# 20. left.
# 21. right.
# 22. participant_id.
# 23. Comments.

I wanna see rows with the participant_id, the group the participant is in (every participants should be assigned to only one group autoatically), the number (no) of the sentence, the item number (the stimuli number), the condition, counterbalancing (y/n) and the left and right values in the experimental dataset (these are taken directly from the csv by pcibex, please return these as well...)

From the rows under the experiments, get me eyetracking calibration events and their timestamp and value, the key events/reading times of the regions (time between r1, r2, r3, r4, r5, r6, r7; every button/key push marks the end of the region's reading), the question RT (the time it took to understand the question), and the Selector's choice selection's time and selection result. These should be the same for practice and live trials.

In this example, the file has data for only one complete submission from a participant named "roro101". So in the dataframe I want to see the this name as participant_id for every event (row). 

I will tell you about eye tracking data later, first let's do the behavioral data.

####

Eye tracking data is in a separate file for each participant, it records gaze times on the left and right canvasses (AOIs, corresponding to the left and right images and consequent choices), please find this one in #roro101.csv. 

There are several data points for every trial, please process these so we can extract key metrics typical for eye-tracking experiments. (You can clean the data first by removing data points where both the left and right canvasses are 0.) The data supposedly shows in miliseconds where did the participant gaze, and the gazed upon AOI will have a 1 value, the ungazed will have 0. 
I want to fist, present this in a meaningful format, so it's easy to see in a progressing timeline, and I want to extract key metrics for each AOI (left and right images).

For example, for each decision event, for each AOI, calculate:
- Dominant AOI: Which AOI (left or right) received more attention overall.
- Fixation Count: Number of times the gaze landed within the AOI.
- Total Dwell Time: Total time spent looking at the AOI.
- Time to First Fixation: How quickly the AOI was noticed.
- Revisit Count: How often participants returned to the AOI.
- Average Fixation Duration: Indicates depth of processing.

It would be amazing if the eyetracking data in the end could be somehow aggregated to the main experiment results dataframe somehow, but not crucial, so that we can inspect all in one place. Can you help me with this? Please see the results file I mentioned.

Later we can compare Across AOIs, and look for patterns by visualizing the data (heatmaps, gaze plots, bar charts, boxplots), interpret the findings, and report the results.

###

Later we can compare Across AOIs
- Use statistical tests (e.g., t-tests, ANOVA) to compare metrics between AOIs.
- Look for patterns like one AOI consistently attracting more attention or faster recognition.
4. Visualize the Data
- Heatmaps: Show where attention was concentrated.
- Gaze Plots / Scanpaths: Reveal the sequence and flow of visual attention.
- Bar Charts or Boxplots: Compare metrics like dwell time or fixation count across AOIs.
5. Interpret the Findings
- Relate metrics to your research questions. For example:
- Did AOI 1 receive more attention because it was more visually salient?
- Was AOI 2 ignored due to poor placement or design?
6. Report the Results
Include:
- Methodology: Describe your eye-tracking setup, participant details, and AOI definitions.
- Descriptive Statistics: Means, standard deviations, etc.
- Inferential Statistics: Significance tests and p-values.
- Visuals: Heatmaps, gaze plots, and comparative charts.
- Discussion: Interpret findings, limitations, and implications.

ðŸ›  Tools You Might Consider
- Gazealytics: A flexible toolkit for visual and comparative gaze analysis.
- iMotions Guide to AOIs offers best practices for defining and analyzing AOIs.

If youâ€™d like, I can help you build a sample report structure or even walk through your dataset. Just let me know what format your data is in or what kind of insights you're hoping to uncover.








Make sure the eye tracking data is cleaned of instances where both the _right_canvas,_left_canvas, are 0, meaning that there was no gaze on neither of them picked up.